# Bigram Language Model

This repository contains a simple implementation of a Bigram Language Model using PyTorch. The model is designed to learn and generate text based on a given dataset, leveraging a transformer architecture with self-attention mechanisms.

## Overview

The Bigram Language Model is a neural network-based text generation model. It uses a transformer architecture to predict the next token in a sequence based on the previous tokens. The model is trained on a text corpus and can generate coherent sequences of text.

## Features

- **Transformer Architecture**: Utilizes multi-head self-attention for capturing dependencies in the text.
- **Custom Dataset Handling**: Reads and processes text data from a provided file.
- **Text Generation**: Generates new text sequences based on the trained model.


