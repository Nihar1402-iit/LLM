# Bigram Language Model

This repository contains a simple implementation of a Bigram Language Model using PyTorch. The model is designed to learn and generate text based on a given dataset, leveraging a transformer architecture with self-attention mechanisms.

## Overview

The Bigram Language Model is a neural network-based text generation model. It uses a transformer architecture to predict the next token in a sequence based on the previous tokens. The model is trained on a text corpus and can generate coherent sequences of text.

## Features

## Features

- **Text Generation**: Capable of generating coherent text sequences based on an initial context.
- **Transformer Architecture**: Utilizes Transformer blocks, including multi-head self-attention and feed-forward layers, to learn complex patterns in text data.
- **Training Data**: Trained on pathology data from `Robbins-Pathologic2005.txt`, allowing the model to generate domain-specific text.
- **Customizable Hyperparameters**: Allows adjustments to various hyperparameters such as batch size, context length, learning rate, etc.


